<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Ziheng Lin | Deep Multi-agent Reinforcement Learning for Traffic Simulation</title>
  <meta name="description" content="# A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <link rel="shortcut icon" href="http://localhost:4000/assets/img/favicon.ico">

  <link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
  <link rel="canonical" href="http://localhost:4000/projects/project_deep_marl/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
      
      <strong>Ziheng</strong> Lin
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon">
          <svg viewBox="0 0 18 15" width="18px" height="15px">
            <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"
            />
            <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"
            />
            <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"
            />
          </svg>
        </span>
      </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="http://localhost:4000/">about</a>

        <!-- Blog -->
        <!-- <a class="page-link" href="http://localhost:4000/blog/">blog</a> -->

        <!-- Pages -->
                 
        <a class="page-link" href="http://localhost:4000/projects/">projects</a>
             

        <!-- CV link -->
        <!-- <a class="page-link" href="http://localhost:4000/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Deep Multi-agent Reinforcement Learning for Traffic Simulation</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content Deep Multi-agent Reinforcement Learning for Traffic Simulation clearfix">
    <p>In everyday life, we use tools such as Google Maps to help us choose the route so we spend the minimum amount of time on the road. The travel time depends on both the traveling distance and the congestion level along the route. The question comes that how we (or robots) can make trade-off between travel distances and congestion level so that the travel time is minimized. In this project, I show how to train agents to make routing decisions using multi-agent deep reinforcement learning.</p>

<h3 id="the-problem">The problem</h3>
<p>Suppose there are many agents located on a map with fixed origins and destinations. For each agent, there are two tasks to learn simultaneously: 1. the best routes between the origin and the destination; 2. the estimated congestion along the route.
The reinforcement learning algorithm is perfect at learning the best travel route when the environment is relatively stable. However, the difficulty comes that each agent is facing many other other agents who are also learning and changing strategies all the time. How do we develop an algorithm that could bring the system to an equilibrium, that is, every agent is satisfied with the choice of route and is not willing to change.</p>

<!-- <img class="three" src="/assets/img/project_deep_marl/rl_diagram.png"> -->
<p>
<em>Figure1: Reinforcement Learning. Each agent receives state-space information and reward from the environment. The agents learns and make adjustments to their maneuver policy.</em>
</p>

<h3 id="multi-agent-neural-fictitious-play">Multi-agent Neural Fictitious Play</h3>
<p>Multi-agent Neural Fictitious Play algorithm solves the problem. The Multi-agent Neural Fictitious Play algorithm requires agents to keep track of other agents’ actions such that the agent is able to react optimally to the opponent’s average strategies. If every agent follows this strategy, the system should converge to an approximate Nash equilibrium. The Neural Fictitious Play algorithm is an extension on the Deep Q-learning algorithm with additional memory that track’s the agent’s average behavior. By combining the optimal strategy (learned by Q-learning) and the average strategy (learned with average behavior), it steer the agents off the greedy behaviors that the entire system would converge to an equilibrium.</p>

<p><img class="three" src="/assets/img/project_deep_marl/algorithm.png" /></p>

<p>Our implementation of Multiagent Neural Fictitious Play algorithm is an extension of the original Neural Fictitious Play by replacing the deep Q-learning with deterministic policy gradient in order to result more stable performance. To do that, we replace the single neural network in Q-learning algorithm with 2 networks: the “actor” network and the “critic” network. The “actor” network computes the probability of actions according to the state space input using a softmax function. The “critic” network gives a score to the state space and action combination (i.e. the Q value). The “critic” network is updated with samples drawn from state spaces, actions and rewards that is stored in the agent’s memory. The “actor” network is updated according to the scores given by the “critic” network. The actor-critic method help agents learn greedy policies very effectively in environments with large state and action space.</p>

<p><img class="three" src="/assets/img/project_deep_marl/rl_neural_networks.png" /></p>
<p>
<em>Figure2: "Actor", "Critic" and "Tracking" neural networks that guide agents' movements.</em>
</p>

<p>Neural Fictitious Play algorithm requires a third network to keep track of the agent’s average strategy, which we call it the “tracking” network. The “tracking” network also uses a softmax function to give probabilities of actions of the agent’s average behavior at each location of the state space. The “tracking” network is updated using supervised learning on historical behavior that is only sampled from the “optimal” policy generated by the actor and critic networks, which is stored in a separated memory.</p>

<p>When an agent needs to make a movement, it first randomly pick either the “optimal” policy or the “average” policy. If the “optimal” policy is picked, the agent follows the movement generated by the “actor” and “critic” network. Otherwise, the agent follows the movement generated by the “tracking” network. We use epsilon-greedy strategy with linearly decreasing epsilon for state space exploration for all agents.</p>

<h3 id="experiments">Experiments</h3>
<p>We performed 2 experiments of multi-agent reinforcement learning on Albany, California traffic network: one with congestion cost and the other without congestion cost. In the one without congestion cost, we use deterministic policy gradient only for faster convergence time. In the one with congestion cost, we added an NFSP component for achieving system equilibrium. In each experiment, there were 50 agents with different origins and destinations on the network. The origins and destinations are randomly distributed on the network. We set the reward of arriving a the destination to be +10. The traveling cost through the links are proportion to the travel distances. For the experiment with congestion cost, the congestion cost is determined as -0.1 per agent traveling through a links at the same time. If 10 agents traveling through a same line, each agent will receive congestion penalty of −0.1 ∗ 10 = −1.0.</p>

<!-- The state space reinforcement learning is defined as the composition for the agent’s current location, origin, location, and other agents’ locations. We use one-hot encoding of nodes on the traffic network to specify location of interest. Thus, as the Albany traffic network has 250 nodes, the state space encoding is a vector of size 250 ∗ 4 = 1000. The action space in our experiment has 9 components that 8 of them specifying different turn type according the angle of turns. The last action is a staying action that allow the agents to wait at a certain location without receiving traveling or congestion penalties.
The neural network architectures for the multi-agent Reinforcement Learning are specified as the following. The 3 neural networks that governs the policy of the agents (actor, critic and tracking networks) all have the same architecture. The input dimension of the networks is dimension of the state space, which is 1000. The output dimension is 9, which are corresponding to the 9 actions defined above. Each of the network has 2 hidden layers with 256 units in each layer. We use RMSProp optimizer and 0.001 initial learning rate for the networks. -->

<p>In the figures below, we show the convergence of Multi-Agent Reinforcement Learning experiments. The vertical axis shows the average rewards earned by all the agents within one episode. We set the initial random exploration (epsilon) to be 90% and it decreases linearly over 700 episodes to final exploration rate at 20%. In the experiment with no congestion cost, shown in red curve, the system came to convergence relatively quicker since each agent only needs to find the “shortest path” (or the most preferred path) to the destination. In the experiment with congestion cost, shown in blue curve, the it took many more episodes before convergence because agents need to explore extensively for the best routing path to avoid traffic. The agent’s opponents are learning and adapting to their strategies as well. This intuition is proven in the congestion plots in Figure 7 and Figure 8. When congestion cost is not introduced, agents routes greedily toward their destinations that many links on the network show relatively high level of congestion. Once congestion costs are introduced and Neural Fictitious Play is applied, all agents are learning both navigation and other agents’ behavior at the same time. Thus, when the system came to an equilibrium, we can see the traffic is relatively evenly distributed on the network.</p>

<p><img class="two" src="/assets/img/project_deep_marl/RL_convergence.png" /></p>
<p>
<em>Figure3: Average agent scores vs. episodes</em>
</p>

<p><img class="two" src="/assets/img/project_deep_marl/network_without_congestion_cost.png" /></p>
<p>
<em>Figure4: Simulation results without congestion cost</em>
</p>

<p>
<img class="two" src="/assets/img/project_deep_marl/network_with_congestion_cost.png" />
</p>
<p>
<em>Figure5: Simulation results with congestion cost</em>
</p>


  </article>

  

  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2018 Ziheng Lin.
    
    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="http://localhost:4000/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="http://localhost:4000/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXXX', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
